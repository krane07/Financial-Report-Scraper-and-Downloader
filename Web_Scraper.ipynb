{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9def9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import re\n",
    "import math\n",
    "import sys \n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72375ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Structure Initialization\n",
    "# Dictionary to store compiled data for all reports\n",
    "compiled_data = {'Document Date': [], 'Contributor': [], 'Headline': [],\n",
    "                 'Language': [], 'Pages': [], 'Tickers': [], 'Company Names': [],\n",
    "                 'Category': [], 'Countries': [], 'Industries': [], 'Regions': [],\n",
    "                 'Subjects': [], 'Report Styles': [], 'Author': [], 'File Name': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5259f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Helper Functions\n",
    "\n",
    "# Function to extract a URL from a script tag using regex\n",
    "def get_url_from_script(source_text, start_marker, end_marker):\n",
    "#    start_marker = 'href=\"'\n",
    "#    end_marker = '\";</script>'\n",
    "    pattern = re.compile(f'{re.escape(start_marker)}(.*?){re.escape(end_marker)}')\n",
    "    match = pattern.search(source_text)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function to download PDF files\n",
    "def downloading_pdfs(p, doc_no):\n",
    "    # Extract document ID from the href attribute\n",
    "    po = re.match(r\"^.*\\((.*)\\).*$\",p['href'])\n",
    "    # Construct the initial URL for document loading\n",
    "    url_1 = f'https://www.example.com/investextsearchlive.php?opt=loadDocument&docid=%5B%22{po.group(1)}%22%5D&doctype=pdf'\n",
    "    # Make a request to the initial URL to get the redirection page\n",
    "    r = requests.get(url_1)\n",
    "    # Parse the redirection page HTML\n",
    "    soup_redirect = bs(r.text, 'html')\n",
    "    # Find the script tag containing the actual PDF URL\n",
    "    script = soup_redirect.find('script')\n",
    "    start_marker = 'href=\"'\n",
    "    end_marker = '\";</script>'\n",
    "    # Extract the actual PDF URL using the helper function\n",
    "    result = get_url_from_script(str(script), start_marker, end_marker)\n",
    "\n",
    "    # Make a request to download the PDF, streaming content\n",
    "    r2 = requests.get(result, stream=True)\n",
    "    # Save the PDF to a file\n",
    "    with open(f'output/{doc_no}.pdf', 'wb') as fd:\n",
    "        for chunk in r2.iter_content(2000):\n",
    "            fd.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b1589b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Selenium WebDriver Initialization (assuming chromedriver.exe is in the path)\n",
    "# Initialize Chrome WebDriver (ensure chromedriver.exe is in your system's PATH or provide full path)\n",
    "driver = webdriver.Chrome(r'path/to/chromedriver.exe') # Placeholder path\n",
    "\n",
    "# Optional: Navigate to the target URL (commented out in original snippet)\n",
    "# driver.get(url) \n",
    "\n",
    "# Get the initial page source\n",
    "html = driver.page_source\n",
    "soup = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6f3b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Extracting Number of Reports and Calculating Iterations\n",
    "\n",
    "# Find the raw string containing the number of reports\n",
    "found_str_raw = soup.find('td', class_ = 'found')\n",
    "# Clean the extracted string\n",
    "found_str = found_str_raw.text.strip()\n",
    "# Search for the integer number within the string using regex\n",
    "search_int = re.search(r'\\b\\d+\\b', found_str)\n",
    "\n",
    "# Process the found number of reports and proceed or exit\n",
    "if search_int:\n",
    "    num_of_reports = int(search_int.group())\n",
    "    # Calculate the number of pages/iterations needed (25 reports per page)\n",
    "    iterations = int(math.ceil(num_of_reports / 25))\n",
    "\n",
    "    # Print the extracted and calculated values\n",
    "    print(num_of_reports)\n",
    "    print(iterations)\n",
    "\n",
    "    # 5. Main Web Scraping Loop (Iterating through pages and extracting data)\n",
    "    ticker_str = 'Tickers'\n",
    "    soup1 = soup # Initialize soup object for iteration\n",
    "\n",
    "    for h in range(iterations):\n",
    "        print(h) # Print current page index\n",
    "        # Find the main data table on the page\n",
    "        tsort_body = soup1.find('table', class_ = 'tablesorter bodyline')\n",
    "    # tsort = tsort_body.select('table.tablesorter:not([class*=\\'bodyline\\'])')\n",
    "        # Find all report rows (even and odd classes)\n",
    "        tr = tsort_body.find_all('tr', class_ = ['even', 'odd'])\n",
    "        # Separate rows into 'additional data' and 'base data' based on pattern\n",
    "        add_data = tr[1::2]\n",
    "        base_data = tr[::2]\n",
    "        # Get the number of reports displayed on the current page\n",
    "        reports_per_pg = len(add_data)\n",
    "\n",
    "        # Loop through each report on the current page\n",
    "        for i in range(reports_per_pg):\n",
    "            # Find header elements within the 'additional data' row\n",
    "            data_header = add_data[i].find_all('b')\n",
    "\n",
    "            # Check if the first header is 'Tickers' (case-insensitive)\n",
    "            if ticker_str.lower() == data_header[0].text.strip().lower():\n",
    "\n",
    "                # Select specific data cells from the 'additional data' row\n",
    "                data = add_data[i].select('td.leftalignment:not([width])')\n",
    "                # Count slashes in the first data cell (likely a filtering condition)\n",
    "                slash_count = data[0].text.strip().count('/')\n",
    "\n",
    "                # Proceed only if no slashes are found\n",
    "                if slash_count == 0:\n",
    "\n",
    "                    # Populate compiled_data with information from additional_data section\n",
    "                    for j, k in zip(data_header, data):\n",
    "                        compiled_data[j.text.strip()].append(k.text.strip())\n",
    "\n",
    "                    # Ensure all lists have consistent lengths by appending 'NA' if data is missing\n",
    "                    if len(compiled_data['Tickers']) > len(compiled_data['Company Names']):\n",
    "                        compiled_data['Company Names'].append('NA') # Fixed typo: 'Comapany Names' to 'Company Names'\n",
    "\n",
    "                    elif len(compiled_data['Tickers']) > len(compiled_data['Category']):\n",
    "                        compiled_data['Category'].append('NA')\n",
    "\n",
    "                    elif len(compiled_data['Tickers']) > len(compiled_data['Countries']):\n",
    "                        compiled_data['Countries'].append('NA')\n",
    "\n",
    "                    elif len(compiled_data['Tickers']) > len(compiled_data['Industries']):\n",
    "                        compiled_data['Industries'].append('NA')\n",
    "\n",
    "                    elif len(compiled_data['Tickers']) > len(compiled_data['Regions']):\n",
    "                        compiled_data['Regions'].append('NA')\n",
    "\n",
    "                    elif len(compiled_data['Tickers']) > len(compiled_data['Subjects']):\n",
    "                        compiled_data['Subjects'].append('NA')\n",
    "\n",
    "                    elif len(compiled_data['Tickers']) > len(compiled_data['Report Styles']):\n",
    "                        compiled_data['Report Styles'].append('NA')\n",
    "\n",
    "                    # Extract data from the 'base data' row\n",
    "                    b_data = base_data[i].find_all('td')\n",
    "                    date = b_data[2].text.strip()\n",
    "                    year = date[-4:] # Extract year from the date\n",
    "                    # Append base data to compiled_data\n",
    "                    compiled_data['Document Date'].append(date)\n",
    "                    compiled_data['Contributor'].append(b_data[3].text.strip())\n",
    "                    compiled_data['Headline'].append(b_data[4].text.strip())\n",
    "                    compiled_data['Author'].append(b_data[5].text.strip())\n",
    "                    compiled_data['Language'].append(b_data[6].text.strip())\n",
    "                    compiled_data['Pages'].append(b_data[7].text.strip())\n",
    "                    # Find the PDF download link\n",
    "                    p = base_data[i].find('a', href=True)\n",
    "                    # Generate a unique document number/filename\n",
    "                    doc_no = year + str(h) + str(i)\n",
    "                    # Call function to download the PDF\n",
    "                    downloading_pdfs(p, doc_no)\n",
    "                    # Record the generated file name\n",
    "                    (compiled_data)['File Name'].append(doc_no)\n",
    "\n",
    "        # 6. Pagination Logic (Navigating to the next page)\n",
    "        # Find pagination links\n",
    "        nxt = soup1.find('span', class_ = 'pagenav').find_all('a', href=True)\n",
    "        # Determine the 'Next' page link based on the number of links found\n",
    "        if len(nxt) > 1:\n",
    "            nxt_pg_ref = nxt[1] #Second link is 'Next'\n",
    "        else:\n",
    "            nxt_pg_ref = nxt[0] #First link is 'Next' (e.g., if only one link exists)\n",
    "    #    nxt_pg_ref = nxt.find('a', href=True)\n",
    "        # Construct the full URL for the next page\n",
    "        url1 = 'https://www.example.com/' + nxt_pg_ref['href']\n",
    "        # Navigate to the next page using Selenium\n",
    "        driver.get(url1)\n",
    "        # Get the HTML source of the new page\n",
    "        html = driver.page_source\n",
    "        # Update the soup object for the next iteration\n",
    "        soup1 =bs(html, 'html.parser')\n",
    "\n",
    "    # 7. Data Export\n",
    "    # Create a Pandas DataFrame from the compiled data\n",
    "    df = pd.DataFrame.from_dict(compiled_data, orient='index').transpose()\n",
    "    # Export the DataFrame to an Excel file\n",
    "    df.to_excel(\"Financial_Analyst_report.xlsx\", index=False)\n",
    "\n",
    "else:\n",
    "    print(\"No reports found. Exiting program.\")\n",
    "    sys.exit(1) # Exit the script with an error code, as num_of_reports is not defined"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
